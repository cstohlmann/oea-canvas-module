{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Canvas Module Ingestion - Pre-Processing\r\n",
        "\r\n",
        "This notebook demonstrates the utility of the OEA_py class notebook, by converting the Canvas tables from record-oriented JSONs to CSVs pre-ingestion. Once any ad hoc column-dtype conversion is complete, the table is overwritten in stage1.\r\n",
        "\r\n",
        "The steps outlined below describe how this notebook is used to convert the Canvas module JSON tables:\r\n",
        "- Set the workspace for where the Canvas tables are to be converted. \r\n",
        "- Read in the original JSONs landed in ```stage1/Transactional/canvas_raw/...```, perform any ad hoc data conversions (e.g. the accounts table needs the parent_account_id column cast to LongType rather than DoubleType) and write the table to stage1 as a CSV: ```stage1/Transactional/canvas/...```\r\n",
        "- 3 function is defined and used:\r\n",
        "   1. **preprocess_canvas_dataset**: main method that reads in the pandas df JSON using the function ```pd.read_json(..., lines=True)```, converts to a spark df and corrects the column-dtypes as needed.\r\n",
        "\r\n",
        "**This notebook may either need updating or removal from pipeline, when processing production data.**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "workspace = 'dev'\r\n",
        "version = '2.0'"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%run OEA_py"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) set the workspace (this determines where in the data lake you'll be writing to and reading from).\r\n",
        "# You can work in 'dev', 'prod', or a sandbox with any name you choose.\r\n",
        "# For example, Sam the developer can create a 'sam' workspace and expect to find his datasets in the data lake under oea/sandboxes/sam\r\n",
        "oea.set_workspace(workspace)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) this step pre-processing the canvas data through reading in the JSONs as records, corrects any schema discepancies and then writes out the df as a CSV in stage1.\r\n",
        "# there is no data transformation happening in this step besides properly reading in the column dtypes properly.\r\n",
        "def preprocess_canvas_dataset(tables_source):\r\n",
        "    items = oea.get_folders(tables_source)\r\n",
        "    for item in items: \r\n",
        "        if item == '_preprocessed_tables':\r\n",
        "            logger.info('Ignoring existing _preprocessed_tables folder.')\r\n",
        "        else:\r\n",
        "            table_path = tables_source +'/'+ item\r\n",
        "            # find the batch data type of the table\r\n",
        "            batch_type_folder = oea.get_folders(table_path)\r\n",
        "            batch_type = batch_type_folder[0]\r\n",
        "            # grab only the latest folder in stage1, used to write the JSON -> CSV to the same rundate folder timestamp\r\n",
        "            # idea is to mimic the same directory structure of tables landed in stage1\r\n",
        "            latest_dt = oea.get_latest_runtime(f'{table_path}/{batch_type}', \"rundate=%Y-%m-%d %H-%M-%S\")\r\n",
        "            latest_dt = latest_dt.strftime(\"%Y-%m-%d %H-%M-%S\")\r\n",
        "            pdf = pd.read_json(oea.to_url(f'{table_path}/{batch_type}/rundate={latest_dt}/*.json'),lines=True)\r\n",
        "            if item == 'submissions':\r\n",
        "                pdf[['graded_anonymously', 'excused']] = pdf[['graded_anonymously', 'excused']].astype(str) # NOTE: df doesn't load properly if array columns aren't cast to strings\r\n",
        "            df = spark.createDataFrame(pdf)\r\n",
        "            # ad hoc step(s) \r\n",
        "            if item == 'accounts':\r\n",
        "                df = df.withColumn('parent_account_id', df['parent_account_id'].cast(LongType()))\r\n",
        "            elif item == 'assignments':\r\n",
        "                df = df.withColumn('submission_types', df['submission_types'].cast(StringType()))\r\n",
        "            elif item == 'quiz_submissions':\r\n",
        "                df = df.withColumn('score', F.round(df['score'], 2)).withColumn('kept_score', F.round(df['kept_score'], 2)).withColumn('score_before_regrade', F.round(df['score_before_regrade'], 2))\r\n",
        "            elif item == 'submissions':\r\n",
        "                df = df.withColumn('quiz_submission_id', df['quiz_submission_id'].cast(LongType())).withColumn('score', F.round(df['score'], 2)).withColumn('published_score', F.round(df['published_score'], 2))\r\n",
        "            else:\r\n",
        "                logger.info(f'no ad hoc processing needed for the Canvas {item} table.')\r\n",
        "            # create the new location for the converted CSVs, and write back to stage1\r\n",
        "            new_table_path = f'stage1/Transactional/canvas/v{version}/{item}/{batch_type}/rundate={latest_dt}'\r\n",
        "            df.coalesce(1).write.save(oea.to_url(f'{new_table_path}'), format='csv', mode='overwrite', header='true', mergeSchema='true')\r\n",
        "            # remove the _SUCCESS file\r\n",
        "            oea.rm_if_exists(new_table_path + '/_SUCCESS', False)\r\n",
        "            logger.info('Pre-processed table: ' + item + ' from: ' + table_path)\r\n",
        "    logger.info('Finished pre-processing Canvas tables')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the version number and pre-process the dataset\r\n",
        "preprocess_canvas_dataset(f'stage1/Transactional/canvas_raw/v{version}')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}