{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Test for processing Canvas data\r\n",
        "\r\n",
        "This notebook demonstrates possible data processing and exploration of the Canvas data, using the OEA_py class notebook. \r\n",
        "\r\n",
        "Most of the data processing done in this notebook are also achieved by executing the Canvas module main pipeline. This notebook is designed as an alternate approach to the same processing, as well as module data exploration and visualization. \r\n",
        "\r\n",
        "The steps are clearly outlined below:\r\n",
        "1. Set the workspace,\r\n",
        "2. Land Canvas Module Higher Ed. Test Data,\r\n",
        "3. Pre-Process Canvas Module Test Data,\r\n",
        "4. Ingest the Canvas Module Test Data,\r\n",
        "5. Refine the Canvas Module Test Data, \r\n",
        "6. Demonstrate Lake Database Queries/Final Remarks, and\r\n",
        "7. Appendix"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%run OEA_py"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) set the workspace (this determines where in the data lake you'll be writing to and reading from).\r\n",
        "# You can work in 'dev', 'prod', or a sandbox with any name you choose.\r\n",
        "# For example, Sam the developer can create a 'sam' workspace and expect to find his datasets in the data lake under oea/sandboxes/sam\r\n",
        "oea.set_workspace('dev')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.) Land Canvas Module Higher Ed. Test Data\r\n",
        "\r\n",
        "Directory: ```GitHub.com (raw data) -> stage1/Transactional/canvas```\r\n",
        "\r\n",
        "The code block below lands 10 OEA Canvas module test data tables, formatted as Canvas Higher Ed. data in your data lake. \r\n",
        "\r\n",
        "Canvas test data tables landed in stage 1:\r\n",
        " 1. **accounts**\r\n",
        " 2. **assignments**\r\n",
        " 3. **assignment_submissions**\r\n",
        " 4. **assignment_submission_summary**\r\n",
        " 5. **courses**\r\n",
        " 6. **course_sections**\r\n",
        " 7. **enrollments**\r\n",
        " 8. **enrollment_terms**\r\n",
        " 9. **roles**\r\n",
        " 10. **users** "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.1) Land batch data files into stage1 of the data lake.\r\n",
        "# In this example we pull Canvas HEd test json data files from github and land it in oea/dev/stage1/Transactional/canvas/v2.0\r\n",
        "import datetime\r\n",
        "currentDate = datetime.datetime.now()\r\n",
        "currentDateTime = currentDate.strftime(\"%Y-%m-%d %H-%M-%S\")\r\n",
        "data = requests.get('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/module_catalog/Canvas/test_data/hed_test_data/accounts.json').text\r\n",
        "oea.land(data, 'canvas/v1.0/accounts', 'accounts_hed_test_data.csv', oea.SNAPSHOT_BATCH_DATA, currentDateTime)\r\n",
        "data = requests.get('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/module_catalog/Canvas/test_data/hed_test_data/courses.json').text\r\n",
        "oea.land(data, 'canvas/v1.0/courses', 'courses_hed_test_data.csv', oea.SNAPSHOT_BATCH_DATA, currentDateTime)\r\n",
        "data = requests.get('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/module_catalog/Canvas/test_data/hed_test_data/course_sections.json').text\r\n",
        "oea.land(data, 'canvas/v1.0/course_sections', 'coursesections_hed_test_data.csv', oea.SNAPSHOT_BATCH_DATA, currentDateTime)\r\n",
        "data = requests.get('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/module_catalog/Canvas/test_data/hed_test_data/roles.json').text\r\n",
        "oea.land(data, 'canvas/v1.0/roles', 'roles_hed_test_data.csv', oea.SNAPSHOT_BATCH_DATA, currentDateTime)\r\n",
        "data = requests.get('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/module_catalog/Canvas/test_data/hed_test_data/users.json').text\r\n",
        "oea.land(data, 'canvas/v1.0/users', 'users_hed_test_data.csv', oea.SNAPSHOT_BATCH_DATA, currentDateTime)\r\n",
        "# normally, these three tables should be landed as delta_batch_data but since functionality is limited for processing delta data, we assume they're snapshot for now.\r\n",
        "data = requests.get('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/module_catalog/Canvas/test_data/hed_test_data/assignment_submission_summary.json').text\r\n",
        "oea.land(data, 'canvas/v1.0/assignment_sumbission_summary', 'assignsubsummary_hed_test_data.csv', oea.SNAPSHOT_BATCH_DATA)\r\n",
        "data = requests.get('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/module_catalog/Canvas/test_data/hed_test_data/enrollments.json').text\r\n",
        "oea.land(data, 'canvas/v1.0/enrollments', 'enrollments_hed_test_data.csv', oea.SNAPSHOT_BATCH_DATA)\r\n",
        "data = requests.get('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/module_catalog/Canvas/test_data/hed_test_data/enrollment_terms.json').text\r\n",
        "oea.land(data, 'canvas/v1.0/enrollment_terms', 'enrollmentterms_hed_test_data.csv', oea.SNAPSHOT_BATCH_DATA)\r\n",
        "\r\n",
        "data = requests.get('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/module_catalog/Canvas/test_data/hed_test_data/assignment_submissions.json').text\r\n",
        "oea.land(data, 'canvas/v1.0/assignment_submissions', 'assignsubmissions_hed_test_data.csv', oea.ADDITIVE_BATCH_DATA)\r\n",
        "data = requests.get('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/module_catalog/Canvas/test_data/hed_test_data/assignments.json').text\r\n",
        "oea.land(data, 'canvas/v1.0/assignments', 'assignments_hed_test_data.csv', oea.ADDITIVE_BATCH_DATA)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.) Pre-Process Canvas Module Test Data\r\n",
        "\r\n",
        "Directory: ```stage1/Transactional/canvas -> stage1/Transactional/canvas```\r\n",
        "\r\n",
        "This step is responsible for pre-processing the Canvas module test data from stage1 back to stage1.\r\n",
        "\r\n",
        "The code blocks in this step read in the original JSON tables using the ```pd.read_json(..., lines=True)``` function, performs any ad hoc data conversions, and writes the table to stage1 as a CSV.\r\n",
        "\r\n",
        "**To-Do's:**\r\n",
        " - Check if this is to be adopted as a standard for JSONs oriented as records (one JSON row represents a row in the df)."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) this step pre-processing the canvas data through reading in the JSONs as records, corrects any schema discepancies and then writes out the df as a CSV in stage1.\r\n",
        "import datetime\r\n",
        "currentDate = datetime.datetime.now()\r\n",
        "currentDateTime = currentDate.strftime(\"%Y-%m-%d %H-%M-%S\")\r\n",
        "\r\n",
        "def clean_data_lake_latest(source_path):\r\n",
        "    \"\"\"only house the latest rundate folder compared to the old data (which were JSONs)\"\"\"\r\n",
        "    latest_folder = oea.get_latest_folder(source_path)\r\n",
        "    items = mssparkutils.fs.ls(oea.to_url(source_path))\r\n",
        "    for item in items:\r\n",
        "        if item.name != latest_folder:\r\n",
        "            logger.info('file removal path: ' + item.path + ' with item: ' + item.name)\r\n",
        "            oea.rm_if_exists(source_path + '/' + item.name)\r\n",
        "            logger.info('Successfully removed folder: ' + item.name + ' from path: ' + item.path)\r\n",
        "        else:\r\n",
        "            logger.info('Kept folder: ' + item.name + ' from path: ' + item.path)\r\n",
        "    logger.info('Finished cleaning data lake to house only the latest folder')\r\n",
        "\r\n",
        "def write_canvas_json_as_csv(write_filepath,batch_type,df):\r\n",
        "    df.coalesce(1).write.save(oea.to_url(f'{write_filepath}/{batch_type}_batch_data/rundate={currentDateTime}'), format='csv', mode='overwrite', header='true', mergeSchema='true')\r\n",
        "\r\n",
        "def preprocess_canvas_dataset(tables_source):\r\n",
        "    items = oea.get_folders(tables_source)\r\n",
        "    for item in items: \r\n",
        "        table_path = tables_source +'/'+ item\r\n",
        "        #batch_type = oea.get_folders(table_path)\r\n",
        "        pdf = pd.read_json(oea.to_url(f'{table_path}/snapshot_batch_data/*/*.json'),lines=True)\r\n",
        "        df = spark.createDataFrame(pdf)\r\n",
        "        if item == 'accounts':\r\n",
        "            df = df.withColumn('parent_account_id', df['parent_account_id'].cast(LongType()))\r\n",
        "            write_canvas_json_as_csv(table_path,'snapshot',df)\r\n",
        "            #clean_data_lake_latest(f'{table_path}/snapshot_batch_data')\r\n",
        "        else:\r\n",
        "            write_canvas_json_as_csv(table_path,'snapshot',df)\r\n",
        "        clean_data_lake_latest(f'{table_path}/snapshot_batch_data')\r\n",
        "        new_table_path = f'{table_path}/snapshot_batch_data/rundate={currentDateTime}'\r\n",
        "        oea.rm_if_exists(new_table_path + '/_SUCCESS', False)\r\n",
        "        #if batch_type == 'snapshot_batch_data':\r\n",
        "        #    pdf = pd.read_json(oea.to_url(f'{table_path}/snapshot_batch_data/*/*.json'),lines=True)\r\n",
        "        #    df = spark.createDataFrame(pdf)\r\n",
        "        #    if item == 'accounts':\r\n",
        "        #        df = df.withColumn('parent_account_id', df['parent_account_id'].cast(LongType()))\r\n",
        "        #        write_canvas_json_as_csv(table_path,'snapshot',df)\r\n",
        "                #clean_data_lake_latest(f'{table_path}/snapshot_batch_data')\r\n",
        "        #    else:\r\n",
        "        #        write_canvas_json_as_csv(table_path,'snapshot',df)\r\n",
        "                #clean_data_lake_latest(f'{table_path}/snapshot_batch_data')\r\n",
        "            # now write to CSV and clean stage1 of the lake\r\n",
        "            #write_canvas_json_as_csv(table_path,'snapshot',df)\r\n",
        "            #clean_data_lake_latest(f'{table_path}/snapshot_batch_data')\r\n",
        "        #elif batch_type == 'delta_batch_data':\r\n",
        "        #    pdf = pd.read_json(oea.to_url(f'{table_path}/delta_batch_data/*/*.json'),lines=True)\r\n",
        "        #    df = spark.createDataFrame(pdf)\r\n",
        "        #    write_canvas_json_as_csv(table_path,'delta',df)\r\n",
        "        #    clean_data_lake_latest(f'{table_path}/delta_batch_data')\r\n",
        "        #elif batch_type == 'additive_batch_data':\r\n",
        "        #    pdf = pd.read_json(oea.to_url(f'{table_path}/additive_batch_data/*/*.json'),lines=True)\r\n",
        "        #    df = spark.createDataFrame(pdf)\r\n",
        "        #    write_canvas_json_as_csv(table_path,'additive',df)\r\n",
        "        #    clean_data_lake_latest(f'{table_path}/additive_batch_data')\r\n",
        "        logger.info('Pre-processed table: ' + item + ' from: ' + table_path)\r\n",
        "    logger.info('Finished pre-processing Canvas tables')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the version number and pre-process the dataset\r\n",
        "version = '2.0'\r\n",
        "preprocess_canvas_dataset(f'stage1/Transactional/canvas/v{version}')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.) Ingest the Canvas Module Test Data\r\n",
        "\r\n",
        "Directory: ```stage1/Transactional/canvas -> stage2/Ingested/canvas```\r\n",
        "\r\n",
        "This step ingests the Canvas module test data from stage1 to stage2/Ingested.\r\n",
        "\r\n",
        "The code blocks in this step ingest the data using the ```oea.ingest()``` function as normal.\r\n",
        "\r\n",
        "**To-Do's:**\r\n",
        " - Check if Canvas test data accurately reflects actual (production) Canvas data."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) The next step is to ingest the batch data into stage2\r\n",
        "# Note that when you run this the first time, you'll see an info message like \"Number of new inbound rows processed: 2\".\r\n",
        "# If you run this a second time, the number of inbound rows processed will be 0 because the ingestion uses spark structured streaming to keep track of what data has already been processed.\r\n",
        "oea.ingest(f'canvas/v2.0/accounts', 'id')\r\n",
        "#oea.ingest(f'canvas/v2.0/assignments', 'id')\r\n",
        "#oea.ingest(f'canvas/v2.0/assignment_submissions', 'anonymous_id')\r\n",
        "#oea.ingest(f'canvas/v2.0/assignment_submission_summary', 'assignment_id')\r\n",
        "oea.ingest(f'canvas/v2.0/courses', 'id')\r\n",
        "oea.ingest(f'canvas/v2.0/course_sections', 'id')\r\n",
        "oea.ingest(f'canvas/v2.0/enrollments', 'id')\r\n",
        "oea.ingest(f'canvas/v2.0/enrollment_terms', 'id')\r\n",
        "oea.ingest(f'canvas/v2.0/roles', 'id')\r\n",
        "oea.ingest(f'canvas/v2.0/users', 'id')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.5) Now you can run queries against the auto-generated \"lake database\" with the ingested Canvas data.\r\n",
        "df = spark.sql(\"select * from ldb_dev_s2i_canvas_v2p0.course_sections\")\r\n",
        "display(df.limit(10))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.) Refine the Canvas Module Test Data\r\n",
        "\r\n",
        "Directory: ```stage2/Ingested/canvas -> stage2/Refined/canvas```\r\n",
        "\r\n",
        "This step then refines the Canvas test data from stage2/Ingested to stage2/Refined, using the metadata.csv. This step is responsible for pseudonymization, which preserves sensitive student information by either hashing or masking the sensitive columns. \r\n",
        "\r\n",
        "Tables are separated into either ```stage2/Refined/canvas/v2.0/general``` or ```stage2/Refined/canvas/v2.0/sensitive```, depending on whether each table is pseudonymized or has a sensitive column-hashing/masking mapping, respectively.\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def refine_canvas(entity_path, metadata=None, primary_key='id'):\r\n",
        "    source_path = f'stage2/Ingested/{entity_path}'\r\n",
        "    primary_key = oea.fix_column_name(primary_key) # fix the column name, in case it has a space in it or some other invalid character\r\n",
        "    path_dict = oea.parse_path(source_path)\r\n",
        "    sink_general_path = path_dict['entity_parent_path'].replace('Ingested', 'Refined') + '/general/' + path_dict['entity']\r\n",
        "    sink_sensitive_path = path_dict['entity_parent_path'].replace('Ingested', 'Refined') + '/sensitive/' + path_dict['entity'] + '_lookup'\r\n",
        "    if not metadata:\r\n",
        "        all_metadata = oea.get_metadata_from_path(path_dict['entity_parent_path'])\r\n",
        "        metadata = all_metadata[path_dict['entity']]\r\n",
        "\r\n",
        "    df_changes = oea.get_latest_changes(source_path, sink_general_path)\r\n",
        "    spark_schema = oea.to_spark_schema(metadata)\r\n",
        "    df_changes = oea.modify_schema(df_changes, spark_schema)        \r\n",
        "\r\n",
        "    if df_changes.count() > 0:\r\n",
        "        df_pseudo, df_lookup = oea.pseudonymize(df_changes, metadata)\r\n",
        "        oea.upsert(df_pseudo, sink_general_path, primary_key) # todo: remove this assumption that the primary key will always be hashed during pseduonymization\r\n",
        "        oea.upsert(df_lookup, sink_sensitive_path, primary_key)    \r\n",
        "        oea.add_to_lake_db(sink_general_path)\r\n",
        "        oea.add_to_lake_db(sink_sensitive_path)\r\n",
        "        logger.info(f'Processed {df_changes.count()} updated rows from {source_path} into stage2/Refined')\r\n",
        "    else:\r\n",
        "        logger.info(f'No updated rows in {source_path} to process.')\r\n",
        "    return df_changes.count()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) this step refines the data through the use of metadata (this is where the pseudonymization of the data occurs).\r\n",
        "def refine_canvas_dataset(tables_source):\r\n",
        "    items = oea.get_folders(tables_source)\r\n",
        "    for item in items: \r\n",
        "        table_path = tables_source +'/'+ item\r\n",
        "        if item == 'metadata.csv':\r\n",
        "            logger.info('ignore metadata processing, since this is not a table to be ingested')\r\n",
        "        else:\r\n",
        "            try:\r\n",
        "                if item == 'accounts':\r\n",
        "                    refine_canvas('canvas/v2.0/accounts', metadata[item], 'id_pseudonym')\r\n",
        "                elif item == 'users':\r\n",
        "                    refine_canvas('canvas/v2.0/users', metadata[item], 'id_pseudonym')\r\n",
        "                else:\r\n",
        "                    refine_canvas('canvas/v2.0/' + item, metadata[item], 'id')\r\n",
        "            except AnalysisException as e:\r\n",
        "                # This means the table may have not been properly refined due to errors with the primary key not aligning with columns expected in the lookup table.\r\n",
        "                pass\r\n",
        "            \r\n",
        "            logger.info('Refined table: ' + item + ' from: ' + table_path)\r\n",
        "    logger.info('Finished refining Canvas tables')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metadata = oea.get_metadata_from_url('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/module_catalog/Canvas/test_data/metadata.csv')\r\n",
        "refine_canvas_dataset('stage2/Ingested/canvas/v2.0')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.) Demonstrate Lake Database Queries/Final Remarks"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# non-hashed primary keys are not automatically added to the lake db - add these tables\r\n",
        "oea.add_to_lake_db('stage2/Refined/canvas/v2.0/general/courses')\r\n",
        "oea.add_to_lake_db('stage2/Refined/canvas/v2.0/general/course_sections')\r\n",
        "oea.add_to_lake_db('stage2/Refined/canvas/v2.0/general/enrollments')\r\n",
        "oea.add_to_lake_db('stage2/Refined/canvas/v2.0/general/enrollment_terms')\r\n",
        "oea.add_to_lake_db('stage2/Refined/canvas/v2.0/general/roles')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) Now you can query the refined data tables in the lake db\r\n",
        "df = spark.sql(\"select * from ldb_dev_s2r_canvas_v2p0.enrollments\")\r\n",
        "display(df)\r\n",
        "df.printSchema()\r\n",
        "df = spark.sql(\"select * from ldb_dev_s2r_canvas_v2p0.users\")\r\n",
        "display(df)\r\n",
        "df.printSchema()\r\n",
        "# You can use the \"lookup\" table for joins (people with restricted access won't be able to perform this query because they won't have access to data in the \"sensitive\" folder in the data lake)\r\n",
        "df = spark.sql(\"select e.course_section_id, e.type, e.workflow_state, u.id_pseudonym, u.name \\\r\n",
        "                from ldb_dev_s2r_canvas_v2p0.enrollments e, ldb_dev_s2r_canvas_v2p0.users u where e.user_id_pseudonym = u.id_pseudonym\")\r\n",
        "display(df.limit(10))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell to reset this example (deleting all the example Canvas data in your workspace)\r\n",
        "oea.rm_if_exists('stage1/Transactional/canvas')\r\n",
        "oea.rm_if_exists('stage2/Ingested/canvas')\r\n",
        "oea.rm_if_exists('stage2/Refined/canvas')\r\n",
        "oea.drop_lake_db('ldb_dev_s2i_canvas_v2p0')\r\n",
        "oea.drop_lake_db('ldb_dev_s2r_canvas_v2p0')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Appendix"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate an initial metadata file for manual modification\r\n",
        "metadata = oea.create_metadata_from_lake_db('ldb_dev_s2i_canvas_v2p0')\r\n",
        "dlw = DataLakeWriter(oea.to_url('stage1/Transactional/canvas'))\r\n",
        "dlw.write('metadata.csv', metadata)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sql db for the ingested Canvas data\r\n",
        "oea.create_sql_db('stage2/Ingested/canvas')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "oea.create_sql_db('stage2/Refined/canvas')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}